# ðŸš¨ Detection Rules Summary - 18+ Critical Monitors

## ðŸ“Š Overview

This project implements **18+ detection rules** specifically designed for LLM applications, with a focus on issues that require **AI Engineer expertise** to resolve.

## ðŸ“‹ Monitor Categories

### 1. **Basic Threshold Monitors** (7 monitors)
**File**: `datadog/monitors.json`

1. **LLM p95 Latency SLO Burn** - Latency exceeding 1500ms
2. **LLM Cost Anomaly Detection** - Cost spike above threshold
3. **LLM Error Burst / Retry Storm** - >10 errors in 5 minutes
4. **LLM Quality Degradation** - Semantic similarity < 0.4
5. **LLM Safety Block Surge** - >5 safety blocks in 10 minutes
6. **LLM Health Score Degradation** - Composite health score < 60
7. **LLM Daily Cost Budget Alert** - Daily cost > $1.00

### 2. **ML-Based Anomaly Detection** (3 monitors)
**File**: `datadog/monitors_anomaly.json`

8. **LLM Cost Anomaly Detection (ML-Based)** - ML detects unusual cost patterns
9. **LLM Latency Anomaly Detection (ML-Based)** - ML detects unusual latency patterns
10. **LLM Quality Degradation Anomaly (ML-Based)** - ML detects unusual quality patterns

### 3. **Advanced Monitors** (4 monitors)
**File**: `datadog/monitors_advanced.json`

11. **Composite: Health Score Degradation with Multiple Failures** - Multiple dimensions failing simultaneously
12. **Predictive: Cost Spike Forecast (6-hour window)** - Forecasts cost spike 6 hours ahead
13. **Workflow Trigger: Latency SLO Breach with Auto-Remediation** - Triggers workflow automation
14. **Workflow Trigger: Cost Spike with Auto-Scale Down** - Triggers workflow automation

**Plus**: Multi-alert grouping for correlated alerts

### 4. **AI Engineer Critical Monitors** (11 monitors) â­ **NEW**
**File**: `datadog/monitors_ai_engineer_critical.json`

These monitors are specifically designed for issues that require **AI engineering expertise**:

15. **Token Usage Explosion (Input/Output Ratio)** - Ratio > 10:1 indicates inefficient prompts
16. **Hallucination Detection (Ungrounded Answers)** - >5 ungrounded answers in 15 minutes
17. **Context Window Exhaustion** - Input tokens > 8000 (approaching limits)
18. **Rate Limiting / Quota Exhaustion** - >3 rate limit errors in 5 minutes
19. **Model Response Consistency Degradation** - High variance in similarity scores
20. **Cost Per Token Efficiency Degradation** - Cost per token above threshold
21. **Prompt Injection Attack Detection** - >2 injection attempts in 10 minutes
22. **Model Switching Frequency (Instability Indicator)** - >5 switches in 30 minutes
23. **Model Availability / Uptime Degradation** - Availability < 95%
24. **Response Time Degradation (Model Processing)** - Processing time > 5000ms
25. **Output Token Truncation (Incomplete Responses)** - >3 truncations in 15 minutes

## ðŸŽ¯ Why These Monitors Matter for AI Engineers

### Token & Context Management
- **Token Usage Explosion**: Identifies inefficient prompt engineering
- **Context Window Exhaustion**: Prevents information loss from truncation
- **Output Token Truncation**: Ensures complete responses

### Quality & Reliability
- **Hallucination Detection**: Catches model generating false information
- **Response Consistency**: Identifies non-deterministic behavior
- **Model Availability**: Ensures service reliability

### Security
- **Prompt Injection Detection**: Catches security attacks
- **Safety Block Surge**: Identifies content policy violations

### Cost & Efficiency
- **Cost Per Token Efficiency**: Identifies inefficient model usage
- **Cost Anomaly Detection**: Catches unexpected cost spikes
- **Daily Cost Budget**: Prevents budget overruns

### Performance & Stability
- **Rate Limiting**: Prevents quota exhaustion
- **Model Switching Frequency**: Identifies routing instability
- **Response Time Degradation**: Catches model performance issues
- **Latency SLO Burn**: Ensures user experience

## ðŸ“ˆ Monitor Statistics

- **Total Monitors**: 18+ detection rules
- **Basic Threshold**: 7 monitors
- **ML-Based Anomaly**: 3 monitors
- **Advanced (Composite/Predictive/Workflow)**: 4 monitors
- **AI Engineer Critical**: 11 monitors
- **Multi-Alert Grouping**: 2 groups

## ðŸ”— Import Instructions

See `DATADOG_IMPORT_GUIDE.md` for step-by-step import instructions.

All monitors include:
- âœ… Clear "What failed?" descriptions
- âœ… "Why did it fail?" root cause analysis
- âœ… "What should the engineer do next?" actionable steps
- âœ… Automatic incident creation
- âœ… Dashboard, logs, and trace attachments
- âœ… Severity levels (SEV-1, SEV-2, SEV-3)

## ðŸ’¡ Innovation Highlights

1. **AI Engineer Focus**: Monitors specifically designed for LLM/AI issues
2. **ML-Based Detection**: Uses Datadog's ML for intelligent anomaly detection
3. **Predictive Alerts**: Forecasts issues before they happen
4. **Composite Monitors**: Multi-dimensional health assessment
5. **Workflow Integration**: Auto-remediation triggers
6. **Multi-Alert Grouping**: Correlates related alerts

## ðŸŽ¯ Coverage

These monitors cover all critical dimensions for LLM applications:
- âœ… **Performance** (latency, processing time)
- âœ… **Reliability** (errors, availability, rate limiting)
- âœ… **Cost** (anomalies, efficiency, budget)
- âœ… **Quality** (hallucinations, consistency, degradation)
- âœ… **Security** (prompt injection, safety blocks)
- âœ… **Efficiency** (token usage, context management)
- âœ… **Stability** (model switching, routing)

---

**Ready for AI Engineers!** These monitors provide comprehensive coverage of all critical LLM application issues. ðŸš€

